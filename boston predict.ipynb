{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1=[[-1.25916633 -0.22786409 -0.43534138 -2.04768835 -1.66449849 -1.07391399\n",
      "  -1.14066584  0.19093933 -1.45049366 -1.48615221]\n",
      " [-0.71111008 -1.22257403  0.74259885  1.28673071  1.40194101 -0.45567242\n",
      "   0.91003857  0.04315571  1.41115646 -1.63911508]\n",
      " [-0.67141923 -0.7831535   0.54716362  0.34507137 -0.72527282  1.53441322\n",
      "  -0.88839283  0.52979263  1.8872834  -0.39570198]\n",
      " [-0.43479885  2.19043685 -1.67661583 -0.39027786 -0.26665942  1.57105597\n",
      "   0.70787698 -0.38611307  1.10559993 -0.94904729]\n",
      " [-2.28660085  1.43290513 -0.46347523 -0.47390862  0.03117246 -0.63790717\n",
      "   1.6533666  -1.18248942 -0.30873282  0.01972568]\n",
      " [ 0.26312872  0.77884305 -0.60811156 -0.3246083  -0.05058766 -1.23482334\n",
      "   0.32035438  1.60128863 -0.21596585  0.11623108]\n",
      " [ 0.00677877 -0.39537815 -0.54698832 -0.43339434  1.75026129 -0.43382733\n",
      "  -1.46574328 -0.99092234  2.26641733 -0.40109036]\n",
      " [ 0.10282172 -1.58268142  1.46768296 -2.28909464  0.60641069 -0.33221962\n",
      "  -0.30803742 -1.58849152  0.75896534  0.70146657]\n",
      " [ 0.02595481  0.08727417  1.68441167  1.0344745  -0.17342521 -0.16362268\n",
      "  -0.58758415  1.82185632  0.20797325 -0.55146849]\n",
      " [ 0.59759212 -1.53212646  1.33257687  0.34422988  0.98503019 -0.34900853\n",
      "   0.30848516 -0.93860933 -1.3047496  -1.2587681 ]\n",
      " [ 1.06002873 -0.03275617  0.50832031  0.33171193 -1.15954418  1.35306034\n",
      "  -0.89428125  0.29722064 -2.64947388 -1.32255429]\n",
      " [ 0.90398025  0.28281339  0.36424318 -0.63309047  0.00486315  0.39756402\n",
      "  -0.79340119  0.12128774 -0.31993811  1.74471598]\n",
      " [-0.21990094 -1.06241507 -0.01849419 -1.36787549 -1.19649521 -1.01926446\n",
      "  -0.9244235  -0.11644774  1.87100375 -0.00324085]] \n",
      " w2=[[ 0.14570179]\n",
      " [ 0.87399602]\n",
      " [ 0.3526955 ]\n",
      " [ 0.71892462]\n",
      " [ 1.96466634]\n",
      " [ 0.84100904]\n",
      " [-0.21535445]\n",
      " [ 2.23126442]\n",
      " [ 0.07580579]\n",
      " [ 1.11989804]]\n",
      "b1=[2.20624998 2.20624998 2.20624998 2.20624998 2.20624998 2.20624998\n",
      " 2.20624998 2.20624998 2.20624998 2.20624998] \n",
      " b2=[0.67529613]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    使用numpy实现Boston房价预测\n",
    "    Step1 数据加载，来源sklearn中的load_boston\n",
    "    Step2 数据规范化，将X 采用正态分布规范化\n",
    "    Step3 初始化网络\n",
    "    Step4 定义激活函数，损失函数，学习率 epoch\n",
    "    Step5 循环执行：前向传播，计算损失函数，反向传播，参数更新\n",
    "    Step6 输出训练好的model参数，即w1, w2, b1, b2\n",
    "\"\"\" \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# 数据加载\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y = data['target']\n",
    "# 将y转化为矩阵的形式\n",
    "y = y.reshape(y.shape[0],1)\n",
    "\n",
    "# 数据规范化\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "\"\"\"\n",
    "    初始化网络参数\n",
    "    定义隐藏层维度，w1,b1,w2,b2\n",
    "\"\"\" \n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "w1 = np.random.randn(n_features, n_hidden)\n",
    "b1 = np.zeros(n_hidden)\n",
    "w2 = np.random.randn(n_hidden, 1)\n",
    "b2 = np.zeros(1)\n",
    "\n",
    "# relu函数\n",
    "def Relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "# 设置学习率\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 定义损失函数\n",
    "def MSE_loss(y, y_hat):\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    return 0.5*np.square(y_hat-y).sum()\n",
    "\n",
    "# 定义线性回归函数\n",
    "def Linear(X, W1, b1):\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    return np.dot(X,W1) + b1\n",
    "\n",
    "# 5000次迭代\n",
    "for t in range(5000):\n",
    "    # 前向传播，计算预测值y (Linear->Relu->Linear)\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    temp = Linear(X_, w1, b1)\n",
    "    temp_relu = Relu(temp)\n",
    "    y_hat = Linear(temp_relu, w2, b2)\n",
    "\n",
    "    # 计算损失函数, 并输出每次epoch的loss\n",
    "    \"\"\" 这里写你的代码 \"\"\"\n",
    "    loss = MSE_loss(y, y_hat)\n",
    "   # print(loss)\n",
    "\n",
    "    # 反向传播，基于loss 计算w1和w2的梯度\n",
    "    \"\"\" 这里写你的代码 \"\"\" \n",
    "    grad_y_hat = 2.0 * (y_hat - y)\n",
    "    grad_w2 = np.dot(temp_relu.T,grad_y_hat)\n",
    "    grad_b2 = np.sum(grad_y_hat)\n",
    "    grad_temp_relu = np.dot(grad_y_hat, w2.T )\n",
    "    grad_temp = grad_temp_relu.copy()\n",
    "    grad_temp_relu[temp<0] = 0\n",
    "    grad_w1 = np.dot(X_.T, grad_temp)\n",
    "    grad_b1 = np.sum(grad_temp_relu)\n",
    "\n",
    "\n",
    "    # 更新权重, 对w1, w2, b1, b2进行更新\n",
    "    \"\"\" 这里写你的代码 \"\"\"  \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    b2 -= learning_rate * grad_b2\n",
    "\n",
    "\n",
    "# 得到最终的w1, w2\n",
    "print('w1={} \\n w2={}'.format(w1, w2))\n",
    "print('b1={} \\n b2={}'.format(b1, b2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
